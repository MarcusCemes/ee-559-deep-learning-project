{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install -U transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIMhq1_fSMpm",
        "outputId": "54699561-6a4c-4fe4-c29c-75f15ce15ee6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNhmk_SpSsn0",
        "outputId": "6c16e8f3-6591-4ef2-9166-d379c23c2e9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM6QjU2dSKMY",
        "outputId": "21dbe642-5a22-4342-9355-0690fbb8234b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hivvbIuKQxa6"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW\n",
        "from collections import defaultdict\n",
        "import time\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Load the BERT model\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "#print(model)\n",
        "\n",
        "# Example of getting the output of the model for a given text\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Use the model in inference mode and classify a give example\n",
        "def classify(text):\n",
        "    inputs = tokenize_text(text)\n",
        "    print(inputs)\n",
        "    outputs = model(**inputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "## Preprocess the TEXT data\n",
        "## Remove HTML tags\n",
        "def remove_html_tags(text):\n",
        "    clean = re.compile('<.*?>')\n",
        "    return re.sub(clean, '', text)\n",
        "\n",
        "\n",
        "## Remove URL\n",
        "def remove_url(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "\n",
        "## Lowercase\n",
        "\n",
        "## Remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "\n",
        "## Handling Contractions using libraries\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "\n",
        "\n",
        "## Remove stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
        "\n",
        "\n",
        "## Lemmatization\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = remove_html_tags(text)\n",
        "    text = remove_url(text)\n",
        "    text = text.lower()\n",
        "    text = remove_punctuation(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = lemmatize_words(text)\n",
        "    return text\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kka0VQikQ3lb",
        "outputId": "8ce9dadf-6b24-4282-e96f-01c89c489d68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Here Starts the Preprocessing\n",
        "\n",
        "class MultilabelHateBert(torch.nn.Module):\n",
        "    def __init__(self,bertmodel):\n",
        "        super(MultilabelHateBert, self).__init__()\n",
        "        self.bertmodel = bertmodel\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, ids):\n",
        "\n",
        "      output_1= self.bertmodel(ids)\n",
        "      output_2 = self.dropout(output_1.pooler_output)\n",
        "      output = self.linear(output_2)\n",
        "      return output\n",
        "\n",
        "multilabel_model = MultilabelHateBert(model)\n",
        "multilabel_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab_Notebooks/Project/base_model_bert_two_epoch.pth', map_location=torch.device('cpu')))\n",
        "print(\"MODEL WEIGHTS LOADED SUCCESFULLY\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX_ugwx3R7ho",
        "outputId": "45797f63-ccd4-47bf-cf12-82f988b365b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL WEIGHTS LOADED SUCCESFULLY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_transform = ['not_hate', 'hate']\n",
        "\n",
        "one = \"omg ego young young inexperienced woman display remarkable self absorbed proud mistake constituent made giving bit power quite surprised find vast majority american think woman represent worst modern america ego control\"\n",
        "two = \"I love how racisim is rising \"\n",
        "three = \"\tYes indeed. She sort of reminds me of the elder lady that played the part in the movie Titanic who was telling her story!!! And I wouldn't have wanted to cover who I really am!! I would be proud!!!! WE should be proud of our race no matter what it is!!\"\n",
        "four = \"@ghostofthor same energy as the YOU DONT LOOK INDIAN STOP LYING ABOUT BEING SOUTH ASIAN cc\t\"\n",
        "five = \"Get rid of them and their anchor babies and grand babies. Fucking worthless guinees \"\n",
        "#not_hate\n",
        "six = \"I'm pretty sure that Atticus is going to die in the sequel.\"\n",
        "seven = \"I don't know why anyone really gives a shit about whether Atticus is racist or not. While racism sucks, the fact remains--there are racist people in the world. I hope our society doesn't become so politically correct that even authors have to fear publishing a book with a certain character for fear of being deemed racist for doing so. Writers write about people. The public--well, we bitch if a piece isn't realistic enough and we bitch if it's too realistic. How can anyone win like this?\"\n",
        "eight = \"He is afghan.  Must be muslim and future enemy? Why caring about it?\"\n",
        "nine = \"The little kid is so cute!\"\n",
        "#hate\n",
        "ten = \"cops like these are the biggest pussies. as a veteran, i assure you it is easy to detain a person without using much force, ESPECIALLY if you have a fucking squad against a single person. I'd like to take one of these guys out back and 'watch him trip and fall'\"\n",
        "eleven = \"Pigs can do this to kids in schools, but PARENTS can beat their  asses at HOME????? Wtf?????\"\n",
        "twelve = \"Fuck u AJ+ we know it's easy to follow Gobbel's law, keep repeating lies on and on and You start feeling it as true, those Hindu pilgrims didn't die of accident in Godhra train massacre, it was a planned conspiracy in which local Muslim leaders having allegiance to Congress party were found to be involved, the key conspirator being arrested just recently, having been absconding for the past 15 years, so check the facts before lying blatantly!!!\"\n",
        "\n",
        "text = twelve\n",
        "\n",
        "print(text)\n",
        "text = preprocess_text(text)\n",
        "print(text)\n",
        "\n",
        "inputs = tokenize_text(text)\n",
        "outputs = multilabel_model(inputs['input_ids'])\n",
        "\n",
        "print(\"Logits:\",outputs)\n",
        "probabilities = F.softmax(outputs, dim=1)\n",
        "print(\"Probs:\", probabilities)\n",
        "print(\"Output:  \", columns_to_transform[torch.argmax(probabilities).item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aRZIQhlR94i",
        "outputId": "3804216a-8583-411f-8725-6e01811fc634"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fuck u AJ+ we know it's easy to follow Gobbel's law, keep repeating lies on and on and You start feeling it as true, those Hindu pilgrims didn't die of accident in Godhra train massacre, it was a planned conspiracy in which local Muslim leaders having allegiance to Congress party were found to be involved, the key conspirator being arrested just recently, having been absconding for the past 15 years, so check the facts before lying blatantly!!!\n",
            "fuck aj know easy follow gobbels law keep repeating lie start feeling true hindu pilgrim die accident godhra train massacre planned conspiracy local muslim leader allegiance congress party found involved key conspirator arrested recently absconding past 15 year check fact lying blatantly\n",
            "Logits: tensor([[-4.2412,  4.5057]], grad_fn=<AddmmBackward0>)\n",
            "Probs: tensor([[1.5893e-04, 9.9984e-01]], grad_fn=<SoftmaxBackward0>)\n",
            "Output:   hate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WELxLbcvS6dz"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}