{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (16.1.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow) (1.26.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: fastparquet in /opt/conda/lib/python3.10/site-packages (2024.5.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fastparquet) (1.26.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2.8.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2023.12.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastparquet) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: contractions in /opt/conda/lib/python3.10/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.10/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/conda/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "!pip install openpyxl\n",
    "!pip install contractions\n",
    "!pip install -U accelerate\n",
    "!pip install -U transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-isnUSZjAx7M"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import  f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmqcGrIIAx7O"
   },
   "source": [
    "##### Import the multilabel data\n",
    "##### [UCBerkeley - Hate Speech Dataset (Multilabel)](https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data locally\n",
    "#https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: (135556, 131)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "## Read the measuring-hate-speech.parquet\n",
    "raw_datasets = pd.read_parquet('measuring-hate-speech.parquet')\n",
    "\n",
    "print(f\"Number of Columns: {raw_datasets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_id', 'annotator_id', 'platform', 'sentiment', 'respect',\n",
       "       'insult', 'humiliate', 'status', 'dehumanize', 'violence',\n",
       "       ...\n",
       "       'annotator_religion_hindu', 'annotator_religion_jewish',\n",
       "       'annotator_religion_mormon', 'annotator_religion_muslim',\n",
       "       'annotator_religion_nothing', 'annotator_religion_other',\n",
       "       'annotator_sexuality_bisexual', 'annotator_sexuality_gay',\n",
       "       'annotator_sexuality_straight', 'annotator_sexuality_other'],\n",
       "      dtype='object', length=131)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "      <th>origin</th>\n",
       "      <th>gender</th>\n",
       "      <th>sexuality</th>\n",
       "      <th>age</th>\n",
       "      <th>disability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: These 4 broads who criticize America...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is about time for all illegals to go back t...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   race  religion  origin  \\\n",
       "0  Yes indeed. She sort of reminds me of the elde...   True     False   False   \n",
       "1  The trans women reading this tweet right now i...  False     False   False   \n",
       "2  Question: These 4 broads who criticize America...  False     False    True   \n",
       "3  It is about time for all illegals to go back t...  False     False    True   \n",
       "4  For starters bend over the one in pink and kic...  False     False   False   \n",
       "\n",
       "   gender  sexuality    age  disability  \n",
       "0   False      False  False       False  \n",
       "1    True      False  False       False  \n",
       "2   False      False  False       False  \n",
       "3   False      False  False       False  \n",
       "4    True      False  False       False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only text and specific targets\n",
    "raw_columns = raw_datasets.columns\n",
    "keep_columns = ['text', 'target_race', 'target_religion', 'target_origin', 'target_gender', 'target_sexuality', 'target_age', 'target_disability']\n",
    "drop_columns = [col for col in raw_columns if col not in keep_columns]\n",
    "\n",
    "raw_datasets = raw_datasets.drop(columns=drop_columns)\n",
    "\n",
    "column_mapping = {column:column.split('_')[1] for column in keep_columns if column.startswith('target')}\n",
    "raw_datasets = raw_datasets.rename(columns=column_mapping)\n",
    "raw_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID2LABEL:\n",
      "{0: 'race', 1: 'religion', 2: 'origin', 3: 'gender', 4: 'sexuality', 5: 'age', 6: 'disability'}\n",
      "\n",
      "LABEL2ID:\n",
      "{'race': 0, 'religion': 1, 'origin': 2, 'gender': 3, 'sexuality': 4, 'age': 5, 'disability': 6}\n"
     ]
    }
   ],
   "source": [
    "# Get two-way label and label ID\n",
    "ID2LABEL = {idx: column for idx, column in enumerate(raw_datasets.columns[1:])}\n",
    "LABEL2ID = {column: idx for idx, column in enumerate(raw_datasets.columns[1:])}\n",
    "\n",
    "print(f\"ID2LABEL:\\n{ID2LABEL}\\n\")\n",
    "print(f\"LABEL2ID:\\n{LABEL2ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "      <th>origin</th>\n",
       "      <th>gender</th>\n",
       "      <th>sexuality</th>\n",
       "      <th>age</th>\n",
       "      <th>disability</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: These 4 broads who criticize America...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is about time for all illegals to go back t...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   race  religion  origin  \\\n",
       "0  Yes indeed. She sort of reminds me of the elde...   True     False   False   \n",
       "1  The trans women reading this tweet right now i...  False     False   False   \n",
       "2  Question: These 4 broads who criticize America...  False     False    True   \n",
       "3  It is about time for all illegals to go back t...  False     False    True   \n",
       "4  For starters bend over the one in pink and kic...  False     False   False   \n",
       "\n",
       "   gender  sexuality    age  disability                               labels  \n",
       "0   False      False  False       False  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "1    True      False  False       False  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  \n",
       "2   False      False  False       False  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "3   False      False  False       False  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "4    True      False  False       False  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to create labels\n",
    "def create_labels(row):\n",
    "    return [float(row[label]) for label in LABEL2ID]\n",
    "\n",
    "raw_datasets['labels'] = raw_datasets.apply(create_labels, axis=1)\n",
    "raw_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7_FEkRzAx7O"
   },
   "source": [
    "### Preprocessing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "An20B7vYAx7O",
    "outputId": "5fad12f8-5cf8-4806-e6e5-f2fccb921b32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Preprocess the TEXT data\n",
    "## Remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "raw_datasets['text'] = raw_datasets['text'].apply(lambda x: remove_html_tags(x))\n",
    "\n",
    "## Remove URL\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "raw_datasets['text'] = raw_datasets['text'].apply(lambda x: remove_url(x))\n",
    "\n",
    "## Lowercase\n",
    "raw_datasets['text'] = raw_datasets['text'].str.lower()\n",
    "\n",
    "## Remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "raw_datasets['text'] = raw_datasets['text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "## Handling Contractions using libraries\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "raw_datasets['text'] = raw_datasets['text'].apply(lambda x: expand_contractions(x))\n",
    "\n",
    "## Remove stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "raw_datasets['text'] = raw_datasets['text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "## Lemmatization\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "raw_datasets['text'] = raw_datasets['text'].apply(lambda x: lemmatize_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fFnGp-_Ax7P"
   },
   "source": [
    "#### Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9Fik-IuMl2Gd"
   },
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "MAX_LEN = 256\n",
    "#TRAIN_BATCH_SIZE = 16\n",
    "#VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2tNG2TgMST05"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: (108444, 9)\n",
      "Validation Dataset: (20334, 9)\n",
      "Test Dataset: (6778, 9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# train (80%), validation (10%), test (10%) split\n",
    "\n",
    "train_data, temp_data = train_test_split(raw_datasets, test_size=0.2, random_state=SEED)\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=0.25, random_state=SEED)\n",
    "\n",
    "print(f\"Train Dataset: {train_data.shape}\")\n",
    "print(f\"Validation Dataset: {validation_data.shape}\")\n",
    "print(f\"Test Dataset: {test_data.shape}\")\n",
    "\n",
    "# Reset the index of the DataFrames\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "validation_data = validation_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Keep only the text and labels columns \n",
    "train_data = train_data[['text', 'labels']]\n",
    "validation_data = validation_data[['text', 'labels']]\n",
    "test_data = test_data[['text', 'labels']]\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "validation_dataset = Dataset.from_pandas(validation_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 108444\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 20334\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 6778\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8dNmnpeaRNEJ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ad4eb517004be8afb31b5de30dbc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ca4aebfbd84e578e6c42c7df086b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba346ca7c7e4216b214b4906f01f3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6778 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "CHECKPOINT = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True, remove_columns=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 108444\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 20334\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 6778\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "dataloaders = {}\n",
    "for dataset_type in tokenized_datasets.keys():\n",
    "    dataloaders[dataset_type] = DataLoader(\n",
    "        dataset=tokenized_datasets[dataset_type],\n",
    "        batch_size=64,\n",
    "        shuffle=(dataset_type == 'train'), \n",
    "        collate_fn=data_collator,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    problem_type='multi_label_classification',\n",
    "    num_labels=len(LABEL2ID),\n",
    "    label2id=LABEL2ID,\n",
    "    id2label=ID2LABEL,\n",
    ")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZnNREnNqdmC"
   },
   "source": [
    "#### Setup Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2vjORRaqfMf",
    "outputId": "817c402a-b881-4cd5-ecca-9829ccc6648b"
   },
   "outputs": [],
   "source": [
    "scheduler_name = 'linear'\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0, no_deprecation_warning=True)\n",
    "num_training_steps = EPOCHS * len(dataloaders['train'])\n",
    "num_warmup_steps = 0\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=scheduler_name,\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.sum(y_true==y_pred) / y_true.size\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    logits, labels = preds\n",
    "    preds_= torch.nn.functional.sigmoid(torch.Tensor(logits))\n",
    "    preds_ = (preds_ >= 0.50).int().numpy()\n",
    "    samples_acc = accuracy_score(labels, preds_)\n",
    "    samples_f1 = f1_score(labels, preds_, average='samples', zero_division=0)\n",
    "    return {\n",
    "        'accuracy': samples_acc,\n",
    "        'f1': samples_f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvLOZaOYq0mI"
   },
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNO3pp7Xq3cj",
    "outputId": "2912b97c-ec76-47ca-b4f6-1176390df3e4"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader):\n",
    "\n",
    "    loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    # set to train mode\n",
    "    model.train()\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch.to(device)\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        predictions = torch.nn.functional.sigmoid(outputs.logits).cpu()\n",
    "        predictions = (predictions >= 0.50).int().numpy()\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs.loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        loss += outputs.loss.item()\n",
    "        train_preds += predictions.tolist()\n",
    "        train_labels += labels.tolist()\n",
    "        \n",
    "    loss /= len(dataloader)\n",
    "    samples_acc = accuracy_score(np.array(train_labels), np.array(train_preds))\n",
    "    samples_f1 = f1_score(np.array(train_labels), np.array(train_preds), average='samples', zero_division=0)\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'accuracy': samples_acc,\n",
    "        'f1': samples_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVFsvSa8sCg1"
   },
   "source": [
    "#### Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7wnDBL3Sr_Zk"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "\n",
    "    loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in tqdm(dataloader):\n",
    "\n",
    "            batch.to(device)\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "            predictions = torch.nn.functional.sigmoid(outputs.logits).cpu()\n",
    "            predictions = (predictions >= 0.50).cpu().numpy()\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            loss += outputs.loss.item()\n",
    "            val_preds += predictions.tolist()\n",
    "            val_labels += labels.tolist()\n",
    "            \n",
    "    # evaluation metrics\n",
    "    loss /= len(dataloader)\n",
    "    samples_acc = accuracy_score(np.array(val_labels), np.array(val_preds))\n",
    "    samples_f1 = f1_score(np.array(val_labels), np.array(val_preds), average='samples', zero_division=0)\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'accuracy': samples_acc,\n",
    "        'f1': samples_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1695/1695 [11:34<00:00,  2.44it/s]\n",
      "100%|██████████| 318/318 [00:50<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train loss: 0.18145 | Validation loss: 0.16350 | Validation accuracy: 0.93565 | Validation F1: 0.82615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1695/1695 [11:33<00:00,  2.45it/s]\n",
      "100%|██████████| 318/318 [00:50<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train loss: 0.15098 | Validation loss: 0.15835 | Validation accuracy: 0.93640 | Validation F1: 0.82746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1695/1695 [11:34<00:00,  2.44it/s]\n",
      "100%|██████████| 318/318 [00:50<00:00,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train loss: 0.13782 | Validation loss: 0.15946 | Validation accuracy: 0.93635 | Validation F1: 0.83097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_metrics = train(model, dataloaders['train'])\n",
    "    validation_metrics = evaluate(model, dataloaders['validation'])\n",
    "          \n",
    "    print(f\"Epoch {epoch+1}\", end=\" | \")\n",
    "    print(f\"Train loss: {train_metrics['loss']:.5f}\", end=\" | \")\n",
    "    print(f\"Validation loss: {validation_metrics['loss']:.5f}\", end=\" | \")\n",
    "    print(f\"Validation accuracy: {validation_metrics['accuracy']:.5f}\", end=\" | \")\n",
    "    print(f\"Validation F1: {validation_metrics['f1']:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_models_weights_dir = 'models_weights_multilabel_bert/'\n",
    "if not os.path.exists(results_models_weights_dir):\n",
    "    os.mkdir(results_models_weights_dir)\n",
    "torch.save(model.state_dict(), results_models_weights_dir + 'multilabel_bert_five_epochs.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMa-pFT4eLAy"
   },
   "outputs": [],
   "source": [
    "#Import\n",
    "#! pip install lime\n",
    "#! pip install transformers-interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwGhn9reeNZC"
   },
   "outputs": [],
   "source": [
    "#samples = [17, 4, 44, 3, 8, 27, 71, 74]\n",
    "#attributions = dict()\n",
    "#attributions['LIME'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLziYQw3eWuD"
   },
   "outputs": [],
   "source": [
    "#import lime\n",
    "#from lime.lime_text import LimeTextExplainer\n",
    "#explainer = LimeTextExplainer(class_names=columns_to_transform, split_expression='\\s+', bow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePv2Rh1F6V1I"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "def predictor(texts):\n",
    "\n",
    "    encodings = tokenizer(texts,\n",
    "                           padding='max_length',\n",
    "                           truncation=True,\n",
    "                           max_length=MAX_LEN,\n",
    "                           return_tensors='pt')\n",
    "\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    token_type_ids = encodings['token_type_ids'].to(device)\n",
    "\n",
    "    logits = multilabel_model(input_ids, attention_mask, token_type_ids)\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "    return probabilities.cpu().detach().numpy()\n",
    "\n",
    "for idx in samples:\n",
    "    instance = val_texts.iloc[idx].text\n",
    "\n",
    "    # Call predictor with the current instance\n",
    "    exp = explainer.explain_instance(instance, predictor, num_features=200, num_samples=64)\n",
    "\n",
    "    explanation_dict = dict(list(exp.as_map().values())[0])\n",
    "    tokens = val_texts.iloc[idx].text.split(' ')\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        scores.append((tokens[i], explanation_dict[i]))\n",
    "\n",
    "    attributions['LIME'].append(scores)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjtOrEylewau"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, transforms\n",
    "\n",
    "# Plotting Code from innvestigate library: https://github.com/albermax/innvestigate\n",
    "def plot_text_heatmap(words, scores, title=\"\", width=10, height=0.2, verbose=0, max_word_per_line=20):\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.set_title(title, loc='left')\n",
    "    tokens = words\n",
    "    if verbose > 0:\n",
    "        print('len words : %d | len scores : %d' % (len(words), len(scores)))\n",
    "\n",
    "    cmap = plt.cm.ScalarMappable(cmap=cm.bwr)\n",
    "    cmap.set_clim(0, 1)\n",
    "\n",
    "    canvas = ax.figure.canvas\n",
    "    t = ax.transData\n",
    "\n",
    "    # normalize scores to the followings:\n",
    "    # - negative scores in [0, 0.5]\n",
    "    # - positive scores in (0.5, 1]\n",
    "    normalized_scores = 0.5 * scores / np.max(np.abs(scores)) + 0.5\n",
    "\n",
    "    if verbose > 1:\n",
    "        print('Raw score')\n",
    "        print(scores)\n",
    "        print('Normalized score')\n",
    "        print(normalized_scores)\n",
    "\n",
    "    # make sure the heatmap doesn't overlap with the title\n",
    "    loc_y = -0.2\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        *rgb, _ = cmap.to_rgba(normalized_scores[i], bytes=True)\n",
    "        color = '#%02x%02x%02x' % tuple(rgb)\n",
    "\n",
    "        text = ax.text(0.0, loc_y, token,\n",
    "                       bbox={\n",
    "                           'facecolor': color,\n",
    "                           'pad': 5.0,\n",
    "                           'linewidth': 1,\n",
    "                           'boxstyle': 'round,pad=0.5'\n",
    "                       }, transform=t)\n",
    "\n",
    "        text.draw(canvas.get_renderer())\n",
    "        ex = text.get_window_extent()\n",
    "\n",
    "        # create a new line if the line exceeds the length\n",
    "        if (i+1) % max_word_per_line == 0:\n",
    "            loc_y = loc_y -  2.5\n",
    "            t = ax.transData\n",
    "        else:\n",
    "            t = transforms.offset_copy(text._transform, x=ex.width+15, units='dots')\n",
    "\n",
    "    if verbose == 0:\n",
    "        ax.axis('off')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVJMgkbFezl9"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Plotting\n",
    "methods = ['LIME']\n",
    "\n",
    "for sample_id in range(len(samples)):\n",
    "    for method in methods:\n",
    "        analysis = attributions[method][sample_id]\n",
    "        words = [t[0] for t in analysis]\n",
    "        scores = np.array([t[1] for t in analysis])\n",
    "        plot_text_heatmap(words, scores, title='Method: %s' % method, verbose=0)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
